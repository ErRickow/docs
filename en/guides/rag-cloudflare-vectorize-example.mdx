---
title: 'RAG with Cloudflare Vectorize'
description: 'Build a RAG application using Neosantara AI and Cloudflare Vectorize'
---

This guide demonstrates how to build a scalable Retrieval-Augmented Generation (RAG) application leveraging **Cloudflare Vectorize** as your managed vector database, integrated with Neosantara AI for embeddings and large language model (LLM) calls.

Cloudflare Vectorize offers a serverless and highly scalable solution for storing and querying vector embeddings, making it an excellent choice for RAG applications that need to handle large datasets.

## Overview

You will learn to:

1. Set up your Neosantara AI API client and Cloudflare API credentials.
2. Create and manage a Vectorize index (v2).
3. Use Neosantara AI's embedding model (`nusa-embedding-0001`) to vectorize your documents.
4. Store these embeddings in Cloudflare Vectorize using NDJSON format.
5. Retrieve relevant documents from Vectorize based on a user query.
6. Use Neosantara AI's chat model (`nusantara-base`) to generate a grounded answer using the retrieved context.

## Prerequisites

* A Cloudflare account with API Token access (with permissions for Vectorize).
* Your Cloudflare Account ID.
* A Neosantara AI API Key.

## Setup

First, install the necessary Python library:

```bash icon="terminal"
pip install -U openai requests
```

### Configure your API Keys and Client

```python icon="python"
import os
import json
import requests
from openai import OpenAI

# --- Neosantara AI Configuration ---
NEOSANTARA_API_KEY = os.getenv("NAI_API_KEY", "YOUR_NEOSANTARA_API_KEY")
NEOSANTARA_BASE_URL = os.getenv("NAI_BASE_URL", "https://api.neosantara.xyz/v1")

neosantara_client = OpenAI(
    base_url=NEOSANTARA_BASE_URL,
    api_key=NEOSANTARA_API_KEY
)

EMBEDDING_MODEL = "nusa-embedding-0001"
CHAT_MODEL = "nusantara-base" # Or "garda-beta-mini" for larger context

# --- Cloudflare Vectorize Configuration ---
CLOUDFLARE_API_TOKEN = os.getenv("CLOUDFLARE_API_TOKEN", "YOUR_CLOUDFLARE_API_TOKEN")
CLOUDFLARE_ACCOUNT_ID = os.getenv("CLOUDFLARE_ACCOUNT_ID", "YOUR_CLOUDFLARE_ACCOUNT_ID")
VECTORIZE_INDEX_NAME = "my-rag-index" # Choose a name for your Vectorize index

# Cloudflare API Base URL for Vectorize v2
CLOUDFLARE_API_BASE = f"https://api.cloudflare.com/client/v4/accounts/{CLOUDFLARE_ACCOUNT_ID}/vectorize/v2/indexes"

headers = {
    "Authorization": f"Bearer {CLOUDFLARE_API_TOKEN}",
    "Content-Type": "application/json"
}
```

## Step 1: Indexing Documents in Vectorize (Ingestion)

First, let\'s define our documents and create a Vectorize index.

### Create Vectorize Index

You need to create an index in Cloudflare Vectorize. The `dimensions` should match the output dimension of your embedding model (e.g., 768 for `nusa-embedding-0001`).

```python icon="python"
def create_vectorize_index(index_name, dimension=768):
    url = CLOUDFLARE_API_BASE
    # v2 uses 'dimensions' instead of 'vector_size'
    data = {"name": index_name, "config": {"dimensions": dimension, "metric": "cosine"}}
    
    print(f"Attempting to create Vectorize index '{index_name}'...")
    response = requests.post(url, headers=headers, json=data)
    
    if response.status_code == 409: # Conflict - index already exists
        print(f"Index '{index_name}' already exists. Skipping creation.")
        return True
    elif response.status_code in [200, 201]:
        print(f"Index '{index_name}' created successfully.")
        return True
    else:
        print(f"Failed to create index: {response.status_code} - {response.text}")
        response.raise_for_status() 
    return False

# Create the index (if it doesn\'t exist)
create_vectorize_index(VECTORIZE_INDEX_NAME)
```

### Your Data

```python icon="python"
documents = [
  {"id": "doc1", "text": "Operating the Climate Control System. Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car."},
  {"id": "doc2", "text": "Our refund policy allows returns within 30 days of purchase. To initiate a refund, please contact our support team at support@neosantara.xyz with your order number and reason for return."},
  {"id": "doc3", "text": "Support hours are Monday to Friday, 9 AM to 5 PM WIB."},
  {"id": "doc4", "text": "Neosantara API rate limit for Free tier is 1000 RPM."} 
]
```

### Generate Embeddings and Upsert to Vectorize

Now, we\'ll embed each document and send it to your Cloudflare Vectorize index using **NDJSON** format.

```python icon="python"
def upsert_to_vectorize(documents_with_ids):
    url = f"{CLOUDFLARE_API_BASE}/{VECTORIZE_INDEX_NAME}/upsert"
    
    ndjson_data = ""
    for doc in documents_with_ids:
        print(f"Embedding document ID: {doc['id']}...")
        embedding_response = neosantara_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=doc['text']
        )
        vector_data = embedding_response.data[0].embedding
        
        # NDJSON format: one JSON object per line
        vector_obj = {
            "id": doc['id'],
            "values": vector_data,
            "metadata": {"text": doc['text']}
        }
        ndjson_data += json.dumps(vector_obj) + "\n"

    print(f"Upserting {len(documents_with_ids)} vectors to Vectorize index '{VECTORIZE_INDEX_NAME}'...")
    
    # Use application/x-ndjson Content-Type for v2 upserts
    upsert_headers = headers.copy()
    upsert_headers["Content-Type"] = "application/x-ndjson"
    
    response = requests.post(url, headers=upsert_headers, data=ndjson_data)
    
    if response.status_code == 200:
        print("Vectors upserted successfully.")
    else:
        print(f"Failed to upsert vectors: {response.status_code} - {response.text}")
        response.raise_for_status()

# Prepare documents with IDs for upserting
documents_for_upsert = [{"id": doc["id"], "text": doc["text"]} for doc in documents]
upsert_to_vectorize(documents_for_upsert)
```

## Step 2: Querying Vectorize (Retrieval)

When a user asks a question, we embed their query and use Vectorize to find the most relevant document embeddings.

```python icon="python"
def query_vectorize(query_text, top_k=2):
    url = f"{CLOUDFLARE_API_BASE}/{VECTORIZE_INDEX_NAME}/query"
    
    # 1. Embed the user\'s query
    print(f"Embedding query: '{query_text}'...")
    embedding_response = neosantara_client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=query_text
    )
    query_vector = embedding_response.data[0].embedding

    # 2. Query Vectorize
    print(f"Querying Vectorize index '{VECTORIZE_INDEX_NAME}' for top {top_k} results...")
    # v2 uses a string for returnMetadata ("all", "indexed", "none")
    data = {"vector": query_vector, "topK": top_k, "returnMetadata": "all"}
    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        results = response.json()
        relevant_passages = []
        for match in results.get('result', {}).get('matches', []):
            if match.get('metadata', {}).get('text'):
                relevant_passages.append(match['metadata']['text'])
        return "\n\n".join(relevant_passages)
    else:
        print(f"Failed to query Vectorize: {response.status_code} - {response.text}")
        response.raise_for_status()
        return "Error retrieving context."
```
## Step 3: Generate Answer (Generation)

Finally, construct the prompt with the retrieved context and send it to a Neosantara AI chat model.

```python icon="python"
def make_prompt(query, retrieved_context):
    processed_context = retrieved_context.replace("\n", " ")
    return f"""
You are a helpful support agent. Answer the user's question using ONLY the provided context.
If the answer isn't in the context, say you don't know.

QUESTION: '{query}'
CONTEXT:
{processed_context}

ANSWER:
"""

def generate_answer_with_rag(query_text):
    # 1. Retrieve relevant context from Vectorize
    context = query_vectorize(query_text)
    
    if "Error retrieving context." in context or not context.strip():
        print("No relevant context found, or an error occurred during retrieval.")
        return neosantara_client.chat.completions.create(
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful support agent. You acknowledge if you cannot find relevant information."},
                {"role": "user", "content": f"QUESTION: '{query_text}'\nCONTEXT: [No relevant information found.]\nANSWER:"}
            ]
        ).choices[0].message.content

    print(f"Retrieved context:\n---\n{context}\n---\n")
    
    # 2. Construct prompt for LLM
    full_prompt = make_prompt(query_text, context)
    
    # 3. Generate answer using Neosantara AI chat model
    response = neosantara_client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "user", "content": full_prompt}
        ]
    )
    return response.choices[0].message.content
```

## Full Example Execution

```python icon="python"
if __name__ == "__main__":
    # Ensure all API keys and Account ID are set
    if "YOUR_NEOSANTARA_API_KEY" in NEOSANTARA_API_KEY or \
       "YOUR_CLOUDFLARE_API_TOKEN" in CLOUDFLARE_API_TOKEN or \
       "YOUR_CLOUDFLARE_ACCOUNT_ID" in CLOUDFLARE_ACCOUNT_ID:
        print("⚠️ Please set your API keys and Cloudflare Account ID environment variables.")
        exit()
    
    # Test Queries
    query1 = "How do I get a refund?"
    print(f"\nUser Query: {query1}")
    answer1 = generate_answer_with_rag(query1)
    print(f"AI Response: {answer1}\n")

    query2 = "What are the climate control features?"
    print(f"\nUser Query: {query2}")
    answer2 = generate_answer_with_rag(query2)
    print(f"AI Response: {answer2}\n")
```
