---
title: 'Context Window'
description: 'Understanding token limits and context windows for Neosantara AI models'
---

The **Context Window** is the maximum amount of information a model can process in a single interaction. It includes both the **Input** (your prompt, instructions, conversation history) and the **Output** (the model's response).

<Card title="Get Your API Key" icon="key" href="https://app.neosantara.xyz/signup" horizontal>
  Sign up now to get **10,000 free credits** and start building with our massive context windows.
</Card>

## Token Limits per Model

Different models have different capacities for "remembering" context. Choosing the right model depends on whether you need to process long documents (high context) or just quick interactions (standard context).

| Model | Context Window (Input) | Max Output Tokens | Best For |
| :--- | :--- | :--- | :--- |
| **Grok 4.1 Fast** | **2,000,000** | 4,096 | Extreme context tasks, finance, agentic workflows |
| **Gemini 3 Flash** | **1,000,000** | 2,400 | High-speed processing with massive context |
| **Claude 4.5 Sonnet** | **200,000** | 4,096 | Complex logic, high-end data processing, coding |
| **Qwen3-32B** | **131,072** | **40,960** | High-speed coding and long documents |
| **Garda-Beta-Mini** | **131,072** | 8,192 | Long document analysis, huge context |
| **Llama-3.3-Nemotron** | **131,072** | 128,000 | Deep reasoning with very long outputs |
| **Sea-Lion v4** | **128,000** | 4,096 | Southeast Asian context, native-level fluency |
| **Nusantara-Base** | **64,000** | 2,048 | General purpose, balanced performance |
| **Archipelago-70B** | 24,000 | 2,048 | Cultural tasks, medium context |

### Understanding the Numbers

* **Context Window (Input)**: The limit for your prompt + conversation history. If you send a prompt that exceeds this limit, the request will fail or be truncated depending on the endpoint configuration.
* **Max Output**: The limit for what the model can generate in one go. Even if a model can read 1M tokens, it has a separate limit for how much it can write in a single response.

## Managing Context

When your conversation grows too long, you might hit the limit. Neosantara provides tools to handle this.

### Automatic Truncation

In the **[Responses API](/api-reference/responses/create)**, you can use the `truncation` parameter:

```json
{
  "model": "nusantara-base",
  "input": "...",
  "truncation": "auto"
}
```

* **`auto`**: The system will automatically drop the oldest messages from the conversation history to fit the new input within the model's limit.
* **`disabled`** (default): The API will return an error if the context limit is exceeded.

### Calculating Usage

You can always check the `usage` field in the API response to see how close you are to the limit:

```json
"usage": {
  "input_tokens": 1024,
  "output_tokens": 50,
  "total_tokens": 1074
}
```

## Need More Context?

If your use case requires processing millions of tokens (e.g., analyzing entire books or codebases), consider using our **RAG (Retrieval-Augmented Generation)** solutions or splitting your content into chunks.

[Contact Sales](mailto:sales@neosantara.xyz) for enterprise options with higher limits.
