---
title: 'Context Window'
description: 'Understanding token limits and context windows for Neosantara AI models'
---

The **Context Window** is the maximum amount of information a model can process in a single interaction. It includes both the **Input** (your prompt, instructions, conversation history) and the **Output** (the model's response).

<Card title="Get Your API Key" icon="key" href="https://app.neosantara.xyz/signup" horizontal>
  Sign up now to get **10,000 free credits** and start building with our massive context windows.
</Card>

## Token Limits per Model

Different models have different capacities for "remembering" context. Choosing the right model depends on whether you need to process long documents (high context) or just quick interactions (standard context).

| Model | Context Window (Input) | Max Output Tokens | Best For |
| :--- | :--- | :--- | :--- |
| **Qwen3-32B** | **131,072** | 8,192 | High-speed coding and huge context |
| **Garda-Beta-Mini** | **131,072** | 8,192 | Long document analysis, huge context |
| **Nusantara-Base** | **64,000** | 2,048 | General purpose, standard context |
| **Llama-3.3-Nemotron** | **131,072** | 4,096 | Complex reasoning with long context |
| **Archipelago-70B** | 24,000 | 2,048 | Cultural tasks, medium context |

### Understanding the Numbers

* **Context Window (Input)**: The limit for your prompt + conversation history. If you send 10,000 tokens of text to `LuminAI` (limit 8k), the request will fail or be truncated.
* **Max Output**: The limit for what the model can generate in one go. Even if `Garda-Beta-Mini` can read 131k tokens, it can "only" write 8k tokens at a time.

## Managing Context

When your conversation grows too long, you might hit the limit. Neosantara provides tools to handle this.

### Automatic Truncation

In the **[Responses API](/api-reference/responses/create)**, you can use the `truncation` parameter:

```json
{
  "model": "nusantara-base",
  "input": "...",
  "truncation": "auto"
}
```

* **`auto`**: The system will automatically drop the oldest messages from the conversation history to fit the new input within the model's limit.
* **`disabled`** (default): The API will return an error if the context limit is exceeded.

### Calculating Usage

You can always check the `usage` field in the API response to see how close you are to the limit:

```json
"usage": {
  "input_tokens": 1024,
  "output_tokens": 50,
  "total_tokens": 1074
}
```

## Need More Context?

If your use case requires processing millions of tokens (e.g., analyzing entire books or codebases), consider using our **RAG (Retrieval-Augmented Generation)** solutions or splitting your content into chunks.

[Contact Sales](mailto:sales@neosantara.xyz) for enterprise options with higher limits.
