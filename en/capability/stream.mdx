---
title: "Streaming"
description: "Enable real-time data flow using Server-Sent Events (SSE) for low-latency user experiences."
---

Streaming allows your application to process and display model outputs as they are being generated, significantly reducing perceived latency. Instead of waiting for the entire response (which could take seconds), your users see the "typing" effect in real-time.

## Protocol Implementation

Neosantara API utilizes **Server-Sent Events (SSE)** to stream data. When you set `stream: true`, the API keeps the connection open and pushes data chunks as they become available.

### The Two Streaming Modes

Neosantara supports two distinct streaming protocols to provide both modern features and legacy compatibility.

<Tabs>
<Tab title="OpenResponses (Recommended)">
The [`/v1/responses`](/en/sdk/responses-api/streaming) endpoint uses a semantic, event-driven protocol. It is designed for complex workflows involving reasoning and tool-calling.

#### Key Event Types
- `response.created`: Initial connection established.
- `response.output_item.added`: A new part of the response (text, reasoning, or tool call) has started.
- `response.output_text.delta`: A snippet of the assistant's message content.
- `response.reasoning_summary_text.delta`: A snippet of the model's internal thought process.
- `response.completed`: The entire generation turn is finished.

#### Example Event Trace
```text
event: response.output_text.delta
data: {"type":"response.output_text.delta","delta":"The capital of "}

event: response.output_text.delta
data: {"type":"response.output_text.delta","delta":"Indonesia is "}
```
</Tab>

<Tab title="OpenAI Compatible">
The [`/v1/chat/completions`](/en/sdk/openai-compat/chat-completions) endpoint follows the classic OpenAI chunking format. It is best suited for existing integrations with tools like LangChain or AutoGPT.

#### Example Chunk Trace
```text
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":"Hello"}}]}
data: [DONE]
```
</Tab>
</Tabs>

---

## Technical Details

### 1. HTTP Headers
To ensure a stable stream, Neosantara sets the following headers:
- `Content-Type: text/event-stream`: Identifies the connection as a long-lived stream.
- `Cache-Control: no-cache`: Prevents intermediary proxies from buffering data.
- `Connection: keep-alive`: Maintains the TCP connection.

### 2. The Stop Signal
Once the generation is complete, the server sends a final `[DONE]` data message or a `response.completed` event. Clients must detect this signal to close the connection and finalize their internal state.

---

## Implementation Guide

<CodeGroup>

```python Responses API
from openai import OpenAI

client = OpenAI(api_key="your_key", base_url="https://api.neosantara.xyz/v1")

# The 'responses' endpoint provides rich event objects
stream = client.responses.create(
    model="nusantara-base",
    input="Write a poem about Jakarta.",
    stream=True
)

for event in stream:
    if event.type == 'response.output_text.delta':
        print(event.delta, end="", flush=True)
```

```javascript Chat Completions
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "your_key",
  baseURL: "https://api.neosantara.xyz/v1"
});

const stream = await openai.chat.completions.create({
  model: "grok-4.1-fast",
  messages: [{ role: "user", content: "Count to 10" }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || "");
}
```

</CodeGroup>

## Best Practices

1. **Avoid Buffering**: Ensure your application or proxy (like Nginx) doesn't buffer responses. Use `X-Accel-Buffering: no` if behind Nginx.
2. **Handle Interruptions**: Network connections can drop. Always wrap your stream iteration in a `try/except` block to gracefully handle timeouts or resets.
3. **UI Feedback**: Use the first few chunks to stop "loading" spinners and show the assistant's avatar to make the app feel faster.
