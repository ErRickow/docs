---
title: "Reasoning"
description: "Leverage advanced cognitive capabilities with step-by-step chain-of-thought processing."
---

import { ModelCards } from '/snippets/model-cards.jsx';

Reasoning models are designed to "think" before they speak. Unlike standard chat models that predict the next token immediately, reasoning models generate an internal chain-of-thought to decompose complex instructions, verify facts, and correct their own logic before presenting a final answer.

## The `reasoning_content` Field

Neosantara provides a unified interface for reasoning across all providers. Regardless of whether a model uses native blocks or specific tags (like `<think>`), Neosantara normalizes this output into a dedicated **`reasoning_content`** field in the API response.

---

## How It Works

Neosantara employs three distinct mechanisms to enable reasoning, depending on the model's capabilities.

### 1. Native Reasoning
Models specifically trained for reasoning, such as **DeepSeek-R1**, **Claude 3.7**, and **Gemini 2.0 Flash Thinking**, utilize their built-in logic. Neosantara connects directly to these native "thinking" protocols to ensure the highest quality of thought.

### 2. Instruction Injection
For standard models that don't have native reasoning (e.g., `nusantara-base`, `archipelago-70b`), Neosantara uses **Instruction Injection**. When reasoning is enabled, the system automatically prepends complex logical instructions to the system prompt, forcing the model to break down its process into steps.

### 3. Reasoning Effort & Budgets
For supported models, you can control the "depth" of thought using the `thinking` parameter.

| Budget (Tokens) | Effort Level | Use Case |
| :--- | :--- | :--- |
| < 2,000 | `minimal` | Quick logical checks, simple summaries. |
| 2,000 - 5,000 | `low` | Basic coding tasks, multi-step instructions. |
| 5,000 - 10,000 | `medium` | Complex debugging, creative writing logic. |
| > 10,000 | `high` | Mathematical proofs, architectural planning. |

### Token Reporting
Reasoning tokens are tracked separately from the final answer to provide better transparency into usage and costs.
*   **OpenAI Format**: Reported in `usage.completion_tokens_details.reasoning_tokens`.
*   **Responses API**: Reported in `usage.output_tokens_details.reasoning_tokens`.
*   **Billing**: Reasoning tokens are billed at the same rate as standard output tokens for most models.

---

## Enabling Reasoning

You can activate reasoning using the `thinking` configuration object or a manual prompt trigger.

### Full Implementation

For complete code examples and specific parameter mapping across different SDKs, refer to:

<CardGroup cols={3}>
  <Card title="Responses API" icon="bolt" href="/en/sdk/responses-api/text-generation#reasoning-models">
    How reasoning blocks appear in the modern stateful output array.
  </Card>
  <Card title="OpenAI Compatible" icon="openai" href="/en/sdk/openai-compat/chat-completions">
    Using the `thinking` parameter with the standard OpenAI SDK.
  </Card>
  <Card title="Anthropic Compatible" icon="bolt" href="/en/sdk/anthropic-compat/messages">
    Native support for the Anthropic thinking configuration object.
  </Card>
</CardGroup>

---

## Best Practices

1. **Use Streaming**: Reasoning can generate thousands of internal tokens before the final answer appears. Use [Streaming](/en/capability/stream) to provide real-time visual feedback ("Thinking...") to your users.
2. **Be Specific, Not Descriptive**: Give reasoning models clear objectives but avoid micromanaging their steps. Let the model determine the optimal path to the solution.
3. **Budget Wisely**: More reasoning tokens increase latency and cost. Only allocate high budgets (>10k) for truly complex scientific or mathematical tasks.

## Supported Models

<ModelCards capability="reasoning" />
