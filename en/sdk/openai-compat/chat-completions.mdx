---
title: "Chat Completions"
description: "Create chat completions using various AI models available through Neosantara AI Gateway."
---

Create chat completions using various AI models available through the Neosantara AI Gateway. This endpoint follows the standard OpenAI Chat Completions specification.

**Endpoint**

`POST /v1/chat/completions`

### Basic chat completion

Create a non-streaming chat completion.

<CodeGroup>

```typescript chat-completion.ts
import OpenAI from 'openai';
 
const openai = new OpenAI({
  apiKey: process.env.NAI_API_KEY,
  baseURL: 'https://api.neosantara.xyz/v1',
});
 
const completion = await openai.chat.completions.create({
  model: 'grok-4.1-fast-non-reasoning',
  messages: [
    {
      role: 'user',
      content: 'Write a one-sentence bedtime story about a unicorn.',
    },
  ],
  stream: false,
});
 
console.log('Assistant:', completion.choices[0].message.content);
console.log('Tokens used:', completion.usage);
```

```python chat-completion.py
import os
from openai import OpenAI
 
client = OpenAI(
    api_key=os.getenv('NAI_API_KEY'),
    base_url='https://api.neosantara.xyz/v1'
)
 
completion = client.chat.completions.create(
    model='grok-4.1-fast-non-reasoning',
    messages=[
        {
            'role': 'user',
            'content': 'Write a one-sentence bedtime story about a unicorn.'
        }
    ],
    stream=False,
)
 
print('Assistant:', completion.choices[0].message.content)
print('Tokens used:', completion.usage)
```

</CodeGroup>

#### Response format

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "grok-4.1-fast-non-reasoning",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Once upon a time, a gentle unicorn with a shimmering silver mane danced through moonlit clouds, sprinkling stardust dreams upon sleeping children below."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 28,
    "total_tokens": 43
  }
}
```

---

### Streaming chat completion

Create a streaming chat completion that streams tokens as they are generated.

<CodeGroup>

```typescript streaming-chat.ts
import OpenAI from 'openai';
 
const openai = new OpenAI({
  apiKey: process.env.NAI_API_KEY,
  baseURL: 'https://api.neosantara.xyz/v1',
});
 
const stream = await openai.chat.completions.create({
  model: 'grok-4.1-fast-non-reasoning',
  messages: [
    {
      role: 'user',
      content: 'Write a one-sentence bedtime story about a unicorn.',
    },
  ],
  stream: true,
});
 
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content);
  }
}
```

```python streaming-chat.py
import os
from openai import OpenAI
 
client = OpenAI(
    api_key=os.getenv('NAI_API_KEY'),
    base_url='https://api.neosantara.xyz/v1'
)
 
stream = client.chat.completions.create(
    model='grok-4.1-fast-non-reasoning',
    messages=[
        {
            'role': 'user',
            'content': 'Write a one-sentence bedtime story about a unicorn.'
        }
    ],
    stream=True,
)
 
for chunk in stream:
    content = chunk.choices[0].delta.content
    if content:
        print(content, end='', flush=True)
```

</CodeGroup>

#### Streaming response format

Streaming responses are sent as [Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events). Each event contains a JSON object with partial response data.

The response format follows the OpenAI streaming specification:

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"grok-4.1-fast-non-reasoning","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}
 
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"grok-4.1-fast-non-reasoning","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}
 
data: [DONE]
```

---

### Image attachments

Send images as part of your chat completion request using multimodal models.

<CodeGroup>

```typescript image-analysis.ts
import fs from 'node:fs';
import OpenAI from 'openai';
 
const openai = new OpenAI({
  apiKey: process.env.NAI_API_KEY,
  baseURL: 'https://api.neosantara.xyz/v1',
});
 
// Read the image file as base64
const imageBuffer = fs.readFileSync('./path/to/image.png');
const imageBase64 = imageBuffer.toString('base64');
 
const completion = await openai.chat.completions.create({
  model: 'claude-3-haiku',
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'Describe this image in detail.' },
        {
          type: 'image_url',
          image_url: {
            url: `data:image/png;base64,${imageBase64}`,
            detail: 'auto',
          },
        },
      ],
    },
  ],
  stream: false,
});
 
console.log('Assistant:', completion.choices[0].message.content);
```

```python image-analysis.py
import os
import base64
from openai import OpenAI
 
client = OpenAI(
    api_key=os.getenv('NAI_API_KEY'),
    base_url='https://api.neosantara.xyz/v1'
)
 
# Read the image file as base64
with open('./path/to/image.png', 'rb') as image_file:
    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')
 
completion = client.chat.completions.create(
    model='claude-3-haiku',
    messages=[
        {
            'role': 'user',
            'content': [
                {'type': 'text', 'text': 'Describe this image in detail.'},
                {
                    'type': 'image_url',
                    'image_url': {
                        'url': f'data:image/png;base64,{image_base64}',
                        'detail': 'auto'
                    }
                }
            ]
        }
    ],
    stream=False,
)
 
print('Assistant:', completion.choices[0].message.content)
```

</CodeGroup>

---

### PDF attachments

Send PDF documents as part of your chat completion request.

<CodeGroup>

```typescript pdf-analysis.ts
import fs from 'node:fs';
import OpenAI from 'openai';
 
const openai = new OpenAI({
  apiKey: process.env.NAI_API_KEY,
  baseURL: 'https://api.neosantara.xyz/v1',
});
 
// Read the PDF file as base64
const pdfBuffer = fs.readFileSync('./path/to/document.pdf');
const pdfBase64 = pdfBuffer.toString('base64');
 
const completion = await openai.chat.completions.create({
  model: 'claude-3-haiku',
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is the main topic of this document? Please summarize the key points.',
        },
        {
          type: 'file',
          file_url: {
            url: `data:application/pdf;base64,${pdfBase64}`,
          },
        },
      ],
    },
  ],
  stream: false,
});
 
console.log('Assistant:', completion.choices[0].message.content);
```

```python pdf-analysis.py
import os
import base64
from openai import OpenAI
 
client = OpenAI(
    api_key=os.getenv('NAI_API_KEY'),
    base_url='https://api.neosantara.xyz/v1'
)
 
# Read the PDF file as base64
with open('./path/to/document.pdf', 'rb') as pdf_file:
    pdf_base64 = base64.b64encode(pdf_file.read()).decode('utf-8')
 
completion = client.chat.completions.create(
    model='claude-3-haiku',
    messages=[
        {
            'role': 'user',
            'content': [
                {
                    'type': 'text',
                    'text': 'What is the main topic of this document? Please summarize the key points.'
                },
                {
                    'type': 'file',
                    'file_url': {
                        'url': f'data:application/pdf;base64,{pdf_base64}'
                    }
                }
            ]
        }
    ],
    stream=False,
)
 
print('Assistant:', completion.choices[0].message.content)
```

</CodeGroup>

<Note>
  **Neosantara Extension**: While the standard OpenAI Chat Completions API primarily supports `image_url`, Neosantara extends this to support `file` types via `file_url` for documents like PDFs. Note that inline base64 attachments shown here are a Neosantara-specific extension.
</Note>

---

### Parameters

#### Required parameters

*   `model` (string): The model ID to use (e.g., `grok-4.1-fast-non-reasoning`, `claude-3-haiku`).
*   `messages` (array): Array of message objects with `role` and `content` fields.

#### Optional parameters

*   `stream` (boolean): Whether to stream the response. Defaults to `false`.
*   `temperature` (number): Controls randomness in the output. Range: 0-2.
*   `max_tokens` (integer): Maximum number of tokens to generate.
*   `top_p` (number): Nucleus sampling parameter. Range: 0-1.
*   `frequency_penalty` (number): Penalty for frequent tokens. Range: -2 to 2.
*   `presence_penalty` (number): Penalty for present tokens. Range: -2 to 2.
*   `stop` (string or array): Stop sequences for the generation.
*   `tools` (array): Array of tool definitions for function calling.
*   `tool_choice` (string or object): Controls which tools are called.
*   `response_format` (object): Controls the format of the model's response (e.g., JSON mode).

---

## Technical Deep Dive

While the Chat Completions endpoint is OpenAI-compatible, Neosantara adds several enhancements and technical optimizations under the hood.

### Capability Guides
For complete technical details on specific features, refer to the following capability pages:

*   **[Streaming Protocol](/en/capability/stream)**: Learn about the underlying SSE structure and retry mechanisms.
*   **[Reasoning & Thinking](/en/capability/reasoning)**: Detailed guide on the `thinking` parameter, native reasoning extraction, and usage reporting.
*   **[Image & Vision](/en/capability/image-understanding)**: Understand SSRF protection, Base64 encoding, and multimodal normalization across providers.
*   **[Structured Outputs](/en/capability/structured)**: Deep dive into JSON Mode, instruction injection, and automated markdown stripping.
*   **[Premium Guardrails](/en/capability/guardrails)**: Technical details on automated PII redaction for Indonesian contexts.
