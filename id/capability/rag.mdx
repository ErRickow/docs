---
title: "Retrieval-Augmented Generation (RAG)"
description: "Bangun AI Sadar Pengetahuan menggunakan Neosantara Embeddings dan Chat"
---

Panduan ini menunjukkan cara membangun aplikasi RAG "Dunia Nyata": **Asisten Dukungan Pelanggan** yang menjawab pertanyaan berdasarkan dokumentasi perusahaan Kamu, bukan hanya data pelatihannya.

## Apa yang kita bangun

Kita akan membangun sistem yang:

1. **Membaca** dokumen dukungan Kamu.
2. **Mengonversi** mereka menjadi vektor embedding menggunakan `nusa-embedding-0001`.
3. **Mencari** info relevan saat pengguna mengajukan pertanyaan.
4. **Menghasilkan** jawaban yang akurat menggunakan `nusantara-base` atau `garda-beta-mini`.

## Prasyarat

* Kunci API Neosantara.
* Penyimpanan vektor sederhana (kita akan menggunakan contoh in-memory lokal untuk kesederhanaan, tetapi Kamu dapat menggunakan Pinecone, Supabase, atau Qdrant).

---

## Langkah 1: Buat Embedding (Ingesti)

Pertama, kita perlu mengubah data teks Kamu menjadi angka (vektor) sehingga AI dapat mencarinya.

```javascript icon="js"
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://api.neosantara.xyz/v1",
  apiKey: "KUNCI_API_NEOSANTARA_KAMU"
});

// "Basis Pengetahuan" Kamu
const documents = [
  "Untuk mereset kata sandi Kamu, buka Pengaturan > Keamanan > Reset Kata Sandi.",
  "Kebijakan pengembalian dana kami mengizinkan pengembalian dalam waktu 30 hari setelah pembelian. Untuk memulai pengembalian dana, silakan hubungi tim dukungan kami di support@neosantara.xyz dengan nomor pesanan dan alasan pengembalian Kamu.",
  "Jam dukungan adalah Senin sampai Jumat, pukul 09.00 hingga 17.00 WIB.",
  "Batas kecepatan API Neosantara untuk tier Gratis adalah 1000 RPM."
];

async function createKnowledgeBase() {
  console.log("Membuat embedding...");
  
  const vectorStore = [];

  for (const doc of documents) {
    const response = await client.embeddings.create({
      model: "nusa-embedding-0001", // Dioptimalkan untuk pengambilan
      input: doc
    });

    vectorStore.push({
      text: doc,
      vector: response.data[0].embedding
    });
  }
  
  return vectorStore;
}
```

## Langkah 2: Cari (Pengambilan)

Saat pengguna mengajukan pertanyaan, kita mengonversi pertanyaan mereka menjadi vektor dan menemukan dokumen yang paling cocok menggunakan **Cosine Similarity**.

```javascript icon="js"
// Fungsi cosine similarity sederhana
function cosineSimilarity(vecA, vecB) {
  let dotProduct = 0.0;
  let normA = 0.0;
  let normB = 0.0;
  for (let i = 0; i < vecA.length; i++) {
    dotProduct += vecA[i] * vecB[i];
    normA += vecA[i] * vecA[i];
    normB += vecB[i] * vecB[i];
  }
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

async function retrieveContext(query, vectorStore) {
  // 1. Embed pertanyaan pengguna
  const queryResponse = await client.embeddings.create({
    model: "nusa-embedding-0001",
    input: query
  });
  const queryVector = queryResponse.data[0].embedding;

  // 2. Hitung kesamaan dengan semua dokumen
  const scoredDocs = vectorStore.map(doc => ({
    text: doc.text,
    score: cosineSimilarity(queryVector, doc.vector)
  }));

  // 3. Ambil 2 dokumen paling relevan
  scoredDocs.sort((a, b) => b.score - a.score);
  return scoredDocs.slice(0, 2).map(d => d.text).join("\n\n");
}
```

## Langkah 3: Hasilkan Jawaban (Generasi)

Terakhir, kita meneruskan konteks yang diambil ke LLM untuk menghasilkan jawaban yang alami.

```javascript icon="js"
async function answerUserQuestion(question, vectorStore) {
  console.log(`Pengguna bertanya: "${question}"`);

  // Ambil info relevan
  const context = await retrieveContext(question, vectorStore);
  console.log(`Menemukan konteks:\n${context}\n---

`);

  // Hasilkan jawaban
  const response = await client.chat.completions.create({
    model: "nusantara-base", // Model tujuan umum
    messages: [
      {
        role: "system",
        content: "Kamu adalah agen dukungan yang membantu. Jawab pertanyaan pengguna HANYA menggunakan konteks yang disediakan. Jika jawabannya tidak ada dalam konteks, katakan Kamu tidak tahu."
      },
      {
        role: "user",
        content: `Konteks:\n${context}\n\nPertanyaan: ${question}`
      }
    ]
  });

  console.log("Jawaban AI:", response.choices[0].message.content);
}
```

## Menjalankan Contoh Lengkap

Menggabungkan semuanya:

```javascript icon="js"
async function main() {
  // 1. Setup
  const kb = await createKnowledgeBase();

  // 2. Ajukan pertanyaan yang ADA di dokumen
  await answerUserQuestion("Bagaimana cara mendapatkan pengembalian dana?", kb);
  
  // 3. Ajukan pertanyaan yang TIDAK ADA di dokumen
  await answerUserQuestion("Bagaimana cara membuat kue?", kb);
}

main();
```

### Output yang Diharapkan

```text
Pengguna bertanya: "Bagaimana cara mendapatkan pengembalian dana?"
Menemukan konteks:
Kebijakan pengembalian dana kami mengizinkan pengembalian dalam waktu 30 hari setelah pembelian. Untuk memulai pengembalian dana, silakan hubungi tim dukungan kami di support@neosantara.xyz dengan nomor pesanan dan alasan pengembalian Kamu.
...
Jawaban AI: Untuk mendapatkan pengembalian dana, harap hubungi tim dukungan kami di support@neosantara.xyz dengan nomor pesanan dan alasan pengembalian Kamu.

Pengguna bertanya: "Bagaimana cara membuat kue?"
Menemukan konteks:
Jam dukungan adalah Senin sampai Jumat... (atau konteks tidak relevan lainnya)
...
Jawaban AI: Saya tidak tahu. Saya hanya dapat membantu dengan pertanyaan dukungan yang berkaitan dengan layanan kami.
```

## Mengapa ini penting?

Tanpa RAG, model mungkin berhalusinasi tentang kebijakan pengembalian dana palsu atau memberikan saran umum. Dengan RAG, ia mematuhi data **Kamu** secara ketat, memberikan informasi yang akurat dan dapat ditindaklanjuti.

---

## Berintegrasi dengan Database Vektor (misalnya, Pinecone)

Untuk aplikasi RAG siap produksi, Kamu memerlukan cara yang skalabel untuk menyimpan dan mengambil embedding Kamu. **Database vektor** seperti Pinecone, Weaviate, Qdrant, atau Postgres dengan `pgvector` dirancang untuk tujuan ini.

Alur inti RAG tetap sama, tetapi Langkah 1 (Buat Embedding) dan Langkah 2 (Cari) akan melibatkan interaksi dengan database vektor pilihan Kamu, alih-alih array in-memory lokal.

### Langkah Tingkat Tinggi untuk Integrasi Pinecone

1. **Siapkan Indeks Pinecone**: Inisialisasi klien Pinecone dan buat indeks dengan `dimensi` yang sesuai (misalnya, 768 untuk `nusa-embedding-0001`).
2. **Upsert Embedding**: Saat membuat basis pengetahuan Kamu, alih-alih memasukkannya ke `vectorStore`, Kamu akan melakukan `upsert` potongan dokumen Kamu dan embedding korespondensinya ke indeks Pinecone Kamu.
3. **Kueri Pinecone**: Untuk pengambilan, embed pertanyaan pengguna lalu gunakan klien Pinecone untuk menanyakan indeks Kamu untuk top-k (misalnya, top 2) vektor yang paling mirip. Pinecone menangani perhitungan kesamaan untuk Kamu.
4. **Ambil Konteks Lengkap**: Dari hasil kueri Pinecone, ekstrak konten teks asli dari dokumen yang relevan untuk diteruskan ke LLM.

```javascript icon="js"
// Contoh (konseptual):
// import { Pinecone } from '@pinecone-database/pinecone';
// const pc = new Pinecone({ apiKey: 'KUNCI_API_NEOSANTARA_KAMU' });
// const index = pc.index('my-rag-index');

async function createPineconeKnowledgeBase(documents) {
  // ... (embed dokumen seperti sebelumnya)
  // await index.upsert(vectors_with_metadata);
}

async function retrievePineconeContext(query) {
  // ... (embed kueri seperti sebelumnya)
  // const queryVector = ...;
  // const queryResult = await index.query({ vector: queryVector, topK: 2 });
  // return queryResult.matches.map(m => m.metadata.text).join("\n\n");
}
```

Dengan memanfaatkan database vektor yang kuat, aplikasi RAG Kamu dapat diskalakan ke jutaan dokumen sambil tetap mempertahankan pengambilan yang cepat dan akurat.

Untuk contoh lengkap yang dapat dijalankan menggunakan ChromaDB, lihat panduan terperinci kami: [Bangun Aplikasi RAG dengan ChromaDB](/id/guides/rag-chromadb-example).
Untuk berintegrasi dengan Cloudflare Vectorize, lihat: [Bangun Aplikasi RAG dengan Cloudflare Vectorize](/id/guides/rag-cloudflare-vectorize-example).