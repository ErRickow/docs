---
title: "Streaming"
description: "Aktifkan aliran data waktu nyata menggunakan Server-Sent Events (SSE) untuk pengalaman pengguna dengan latensi rendah."
---

Streaming memungkinkan aplikasi kamu untuk memproses dan menampilkan output model saat sedang dibuat, secara signifikan mengurangi latensi yang dirasakan pengguna. Alih-alih menunggu seluruh respons selesai (yang bisa memakan waktu beberapa detik), pengguna kamu akan melihat efek "mengetik" secara waktu nyata.

## Implementasi Protokol

Neosantara API menggunakan **Server-Sent Events (SSE)** untuk mengalirkan data. Saat kamu menyetel `stream: true`, API menjaga koneksi tetap terbuka dan mengirimkan potongan data (chunks) segera setelah tersedia.

### Dua Mode Streaming

Neosantara mendukung dua protokol streaming yang berbeda untuk menyediakan fitur modern sekaligus menjaga kompatibilitas legacy.

<Tabs>
<Tab title="OpenResponses (Direkomendasikan)">
Endpoint [`/v1/responses`](/id/sdk/responses-api/streaming) menggunakan protokol berbasis event yang semantik. Protokol ini dirancang untuk alur kerja kompleks yang melibatkan penalaran (reasoning) dan pemanggilan tool.

#### Tipe Event Utama
- `response.created`: Koneksi awal berhasil dibuat.
- `response.output_item.added`: Bagian baru dari respons (teks, penalaran, atau tool call) telah dimulai.
- `response.output_text.delta`: Potongan konten pesan dari asisten.
- `response.reasoning_summary_text.delta`: Potongan proses berpikir internal model.
- `response.completed`: Seluruh putaran generasi telah selesai.

#### Contoh Trace Event
```text
event: response.output_text.delta
data: {"type":"response.output_text.delta","delta":"Ibukota "}

event: response.output_text.delta
data: {"type":"response.output_text.delta","delta":"Indonesia adalah "}
```
</Tab>

<Tab title="Kompatibel OpenAI">
Endpoint [`/v1/chat/completions`](/id/sdk/openai-compat/chat-completions) mengikuti format chunk klasik OpenAI. Mode ini paling cocok untuk integrasi yang sudah ada dengan alat seperti LangChain atau AutoGPT.

#### Contoh Trace Chunk
```text
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":"Halo"}}]}
data: [DONE]
```
</Tab>
</Tabs>

---

## Detail Teknis

### 1. HTTP Headers
Untuk memastikan aliran yang stabil, Neosantara menyetel header berikut:
- `Content-Type: text/event-stream`: Mengidentifikasi koneksi sebagai aliran data jangka panjang.
- `Cache-Control: no-cache`: Mencegah proxy perantara melakukan buffering data.
- `Connection: keep-alive`: Menjaga koneksi TCP tetap aktif.

### 2. Sinyal Berhenti (Stop Signal)
Setelah generasi selesai, server akan mengirimkan pesan data final `[DONE]` atau event `response.completed`. Klien harus mendeteksi sinyal ini untuk menutup koneksi dan memfinalisasi status internal mereka.

---

## Panduan Implementasi

<CodeGroup>

```python Responses API
from openai import OpenAI

client = OpenAI(api_key="kunci_kamu", base_url="https://api.neosantara.xyz/v1")

# Endpoint 'responses' memberikan objek event yang kaya informasi
stream = client.responses.create(
    model="nusantara-base",
    input="Tulis puisi tentang Jakarta.",
    stream=True
)

for event in stream:
    if event.type == 'response.output_text.delta':
        print(event.delta, end="", flush=True)
```

```javascript Chat Completions
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "kunci_kamu",
  baseURL: "https://api.neosantara.xyz/v1"
});

const stream = await openai.chat.completions.create({
  model: "grok-4.1-fast",
  messages: [{ role: "user", content: "Hitung sampai 10" }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || "");
}
```

</CodeGroup>

## Praktik Terbaik

1. **Hindari Buffering**: Pastikan aplikasi atau proxy kamu (seperti Nginx) tidak menahan (buffer) respons. Gunakan `X-Accel-Buffering: no` jika berada di balik Nginx.
2. **Tangani Interupsi**: Koneksi jaringan bisa terputus. Selalu bungkus iterasi stream kamu dalam blok `try/except` untuk menangani timeout atau reset secara elegan.
3. **Feedback UI**: Gunakan beberapa chunk pertama untuk menghentikan animasi "loading" dan tampilkan avatar asisten agar aplikasi terasa lebih cepat.
